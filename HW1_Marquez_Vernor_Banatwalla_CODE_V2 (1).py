# -*- coding: utf-8 -*-
"""HW1_6373.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZm9RQHPZ5hJO7q5IqCMpIcnA52rxShD

**Start:Downloading required packages**

> Indented block
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import io
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import losses, optimizers
from tensorflow.keras import callbacks
from tensorflow.keras.models import load_model
from google.colab import files

"""**End : Downloading required Packages**

**Start: Import Data**
"""

uploaded=files.upload()

"""**End: Import Data**

**Start: Reading the data and splitting into test and train**
"""

df = pd.read_excel('HW1_Marquez_Vernor_Banatwalla_FinalData_V2.xlsx')
df.head()

X = df.drop(['Date','T','Y(T)'],axis=1).values
y = df['Y(T)'].values

#Train and Test Split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)

#making a copy of xtest and ytest, will need this later
X_testcopy=X_test
y_testcopy=y_test

#####
#X_testcopy[i][1] for all i's in test set gives us price of GM(T)
#y_testcopy[i] for all is gives us Y(T)
#need to compare with  best_modelh2.predict(X_test)<--- gives us prediction for y(t)
####

"""**END: Reading the data and splitting into test and train sets**

**Start: Normalizing the data**
"""

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

X_train.shape

"""**END: Normalizing the data**

**Start: PCA**
"""

pca = PCA(n_components=2)
pca.fit_transform(X)
pca.explained_variance_ratio_
pca.explained_variance_ratio_.cumsum()

print(pca.components_)

pca = PCA().fit(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.savefig('pca.png')

np.cumsum(pca.explained_variance_ratio_)

"""**END:PCA h95 =2**

**4 h values: 2,20,40,55**

**Start:Model h95=2**
"""

modelh2 =Sequential()
modelh2.add(Dense(2, activation="relu", input_shape=(16,)))
modelh2.add(Dense(1, activation="relu"))

modelh2.summary()

plot_model(modelh2)

#weights matrix for model h95=2
modelh2.layers[0].get_weights()

modelh2.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())

#For recording training loss over the whole training set at the end of each epoch

class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())

MyMonitor = MyHistory()

# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h2',monitor='val_loss',save_best_only=True)

"""Training and Predicting"""

#Train the model and store validation loss history in Monitor
# Value of epochs should vary according to your own training results.
Monitor = modelh2.fit(X_train,y_train,epochs=100,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer])

#Train loss vs Val_loss for h=2
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
print(type(train_loss))

""" **Start: converting MSE to rmse/avg(stock)**"""

# Average of gm stock
AvgGm=df['Y(T)'].mean()

# Converting MSe too RMSE/Avg(GM) 
train_loss1=list(np.sqrt(train_loss))
tlavggm=[x/AvgGm for x in train_loss1]
val_loss1=list(np.sqrt(val_loss))
vlavggm=[x/AvgGm for x in val_loss1]

"""**End: converting MSE to rmse/avg(stock)**

**Start: Plot for h2**
"""

train_loss1 = tlavggm[0:50]
val_loss1 = vlavggm[0:50]

train_loss2 = tlavggm[50:1000]
val_loss2 = vlavggm[50:1000]

train_loss3 = tlavggm[1000:4000]
val_loss3 = vlavggm[1000:4000]

train_loss4 = tlavggm[4000:8000]
val_loss4 = vlavggm[4000:8000]

fig, (ax1, ax2,ax3,ax4) = plt.subplots(1, 4,figsize=(20,6))
fig.suptitle('')
x1=range(0,50)
x2 = range(50,1000)
x3= range(1000,4000)
x4 = range(4000,8000)
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)
ax3.plot(x3, train_loss3)
ax3.plot(x3, val_loss3)
ax4.plot(x4, train_loss4)
ax4.plot(x4,val_loss4)
xc=range(8000)

"""**END: Plots for h2**"""

modelh2.evaluate(X_test,y_test)

best_modelh2 =load_model('BestModel.h2')
best_modelh2.evaluate(X_test,y_test)

#Prediction on the test set
ypred=best_modelh2.predict(X_test)

"""**Starting to compute real world increase or decrease using a binary list, 0 mean stock price decreased 1 means it increased**"""

# gives the vector With gm prices on day t as a list
x_testgmlist=[]
for i in range(len(y_test)):
  x_testgmlist.append(X_testcopy[i][1])
  
  i=i+1

#gives the vector with GM prices on day T+1 as a list
y_testlist=list(y_test)
# Subtracts the GM prices on day t from day t-1, gives the difference in prices each day as a list, the difference is stores in list *realdiff*
realdiff=[]
for i in range(len(y_test)):
  x=y_testlist[i] - x_testgmlist[i]
  realdiff.append(x)
  i=i+1

# convert the list "realDiff" to binary with it being 1 if it increases and it being 0 if the price decreases
realdiffbin=[]
for i in range(len(realdiff)):
  if realdiff[i]>0:
    x=1
  else:
    x=0
  realdiffbin.append(x)
  i=i+1
#realdiffbin = binary list for weather the price went up or down on day T+1

#Changes the prediction array to list for day t+1 computes the predicted difference in stock price compared to the price on day before the list" pred diff" shows the predicted price change
y_predlist=ypred.tolist()
preddiff=[]
for i in range(len(y_test)):
  x=list(y_predlist[i] - x_testgmlist[i])
  preddiff.append(x)
  i=i+1

# converts predicted price change to binary, if predicted price was higher then day t+1 return 1 else returns 0
preddiffbin=[]
for i in range(len(preddiff)):
  if preddiff[i][0]>0:
    x=1
  else:
    x=0
  preddiffbin.append(x)
  i=i+1
#preddiffbin dinary list of predictions out model made, 1 means increase, 0 means decrease

## total of the predictions and actual, if total = 0 or 2 prediction was correct, if total equals to 1 it was wrong

totalpredact=[]

for i in range(len(preddiffbin)):
  x=preddiffbin[i]+realdiffbin[i]
  totalpredact.append(x)
  i=i+1
totalpredact.count(1)

#Accuracy for h=2
(len(totalpredact)-totalpredact.count(1))/len(totalpredact)

"""**starting to conduct mlp manually**


"""

optimizer=optimizers.Adam(learning_rate=0.01)
loss_fn=losses.MeanSquaredError()

modelh2.compile(optimizer=optimizer,loss=loss_fn)

batch_size=25

train_dataset= tf.data.Dataset.from_tensor_slices((X_train, y_train))

train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
batch_loss=[]
gradsNorm=[]
epoch_loss=[]
val_loss=[]
best_val_loss=modelh2.evaluate(X_test,y_test,verbose=0)
epochs=2000

best_val_loss

for epoch in range(epochs):
  print("\nStart of epoch %d" %(epoch,))
  for step, (x_batch_train, y_batch_train) in enumerate (train_dataset):
    with tf.GradientTape() as tape:
      y_batch_pred=modelh2(x_batch_train, training=True)
      loss_value=loss_fn(y_batch_train,y_batch_pred)
    #gradients
    grads =tape.gradient(loss_value, modelh2.trainable_weights)
    #norm of grads
    gradsNorm.append( np.sqrt( sum( [np.sum(np.square(g.numpy())) for g in grads])))
    batch_loss.append(loss_value.numpy())
    optimizer.apply_gradients( zip(grads,modelh2.trainable_weights))
  epoch_loss_value= modelh2.evaluate(X_train,y_train,verbose=0)
  epoch_loss.append(epoch_loss_value)

  val_loss_value = modelh2.evaluate(X_test,y_test,verbose=0)
  val_loss.append(val_loss_value)
  print("epoch_loss: %.4f - val_loss:%.4f" %(float(epoch_loss_value),float(val_loss_value)))

  if val_loss_value < best_val_loss:
    best_val_loss=val_loss_value
    modelh2.save("bestModelh2")

grads

len(batch_loss)

np.array(gradsNorm)

np.array(epoch_loss)

bestModelh2 = tf.keras.models.load_model('BestModel.h2')

modelh2.evaluate(X_test,y_test)

bestModelh2.evaluate(X_test,y_test)
#3.6270992755889893

#h* hidden layer activities
bestModelh2.layers[0](X_test).numpy()

import statistics
from statistics import mean
mean(Monitor.history['val_loss'])
#121.77533928235371

"""**Start:Plots for h=2**"""

# insert plots here please



plt.plot(MyMonitor.loss,label="train")
plt.plot(Monitor.history['val_loss'],label="test")
plt.legend()
plt.savefig('h2_val_loss.png')
fig, ax = plt.subplots(2, figsize= (10,6))
ax[0].scatter(range(0,len(gradsNorm)),gradsNorm)
ax[0].set_ylabel('MSE')
ax[1].scatter(range(0,len(batch_loss)), batch_loss)
plt.xlabel('Epoch')
ax[1].set_ylabel('Norm of Gradient MSE')
plt.savefig('h2_batch_labeled.png')



#batch to batch evolution of MSE ???
a = np.array(batch_loss)
plt.plot(np.arange(a.size), a)
plt.xlabel('Iteration No.')
plt.ylabel('Mean Square Error')
plt.title('Gradient Descent on Data (Batch Version)')
plt.show()

#batch to batch evolution of gradient MSE ???
b = np.array(gradsNorm)/4
plt.plot(np.arange(b.size), b)
plt.xlabel('Iteration No.')
plt.ylabel('Gradient Mean Square Error')
plt.show()

train_mse = np.array(val_loss)
test_mse = np.array(epoch_loss)
plt.plot(train_mse, label = 'Train MSE')
plt.plot(test_mse, label = 'Test MSE')
plt.legend(loc="upper right")
plt.show()

"""**END:Plots for h=2**

**Start:Model h=20**
"""

modelh20 =Sequential()
modelh20.add(Dense(20, activation="relu", input_shape=(16,)))
modelh20.add(Dense(1, activation="relu"))

modelh20.summary()

plot_model(modelh20)

#weights matrix for model h=20
modelh20.layers[0].get_weights()

modelh20.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())

#For recording training loss over the whole training set at the end of each epoch

class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())

MyMonitor = MyHistory()

# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h20',monitor='val_loss',save_best_only=True)

#Train the model and store validation loss history in Monitor
# Value of epochs should vary according to your own training results.
Monitor = modelh20.fit(X_train,y_train,epochs=8000,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])

#Train loss vs Val_loss for h=20
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(8000)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.savefig('h20_val_loss.png')

# Average of gm stock
AvgGm=df['Y(T)'].mean()
# Converting MSe too RMSE/Avg(GM) 
train_loss1=list(np.sqrt(train_loss))
tlavggm=[x/AvgGm for x in train_loss1]
val_loss1=list(np.sqrt(val_loss))
vlavggm=[x/AvgGm for x in val_loss1]
train_loss1 = tlavggm[0:50]
val_loss1 = vlavggm[0:50]
train_loss2 = tlavggm[50:1000]
val_loss2 = vlavggm[50:1000]
train_loss3 = tlavggm[1000:4000]
val_loss3 = vlavggm[1000:4000]
train_loss4 = tlavggm[4000:8000]
val_loss4 = vlavggm[4000:8000]
fig, (ax1, ax2,ax3,ax4) = plt.subplots(1, 4,figsize=(20,6))
fig.suptitle('')
x1=range(0,50)
x2 = range(50,1000)
x3= range(1000,4000)
x4 = range(4000,8000)
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)
ax3.plot(x3, train_loss3)
ax3.plot(x3, val_loss3)
ax4.plot(x4, train_loss4)
ax4.plot(x4,val_loss4)
xc=range(8000)

modelh20.evaluate(X_test,y_test)
#2.1908676624298096

best_modelh20 =load_model('BestModel.h20')
best_modelh20.evaluate(X_test,y_test)

ypred=best_modelh20.predict(X_test)



x_testgmlist=[]
for i in range(len(y_test)):
  x_testgmlist.append(X_testcopy[i][1])
  
  i=i+1

y_testlist=list(y_test)
realdiff=[]
for i in range(len(y_test)):
  x=y_testlist[i] - x_testgmlist[i]
  realdiff.append(x)
  i=i+1

realdiffbin=[]
for i in range(len(realdiff)):
  if realdiff[i]>0:
    x=1
  else:
    x=0
  realdiffbin.append(x)
  i=i+1
#realdiffbin = binary list for weather the price went up or down on day T+1

y_predlist=ypred.tolist()
preddiff=[]
for i in range(len(y_test)):
  x=list(y_predlist[i] - x_testgmlist[i])
  preddiff.append(x)
  i=i+1

preddiffbin=[]
for i in range(len(preddiff)):
  if preddiff[i][0]>0:
    x=1
  else:
    x=0
  preddiffbin.append(x)
  i=i+1
#preddiffbin dinary list of predictions out model made, 1 means increase, 0 means decrease

## total of the predictions and actual, if total = 0 or 2 prediction was correct, if total equals to 1 it was wrong

totalpredact=[]

for i in range(len(preddiffbin)):
  x=preddiffbin[i]+realdiffbin[i]
  totalpredact.append(x)
  i=i+1
totalpredact.count(1)

#Accuracy for h=20
(len(totalpredact)-totalpredact.count(1))/len(totalpredact)

optimizer=optimizers.Adam(learning_rate=0.01)
loss_fn=losses.MeanSquaredError()

modelh20.compile(optimizer=optimizer,loss=loss_fn)

batch_size=25

train_dataset= tf.data.Dataset.from_tensor_slices((X_train, y_train))

train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
batch_loss=[]
gradsNorm=[]
epoch_loss=[]
val_loss=[]
best_val_loss=modelh20.evaluate(X_test,y_test,verbose=0)
epochs=2000

for epoch in range(epochs):
  print("\nStart of epoch %d" %(epoch,))
  for step, (x_batch_train, y_batch_train) in enumerate (train_dataset):
    with tf.GradientTape() as tape:
      y_batch_pred=modelh20(x_batch_train, training=True)
      loss_value=loss_fn(y_batch_train,y_batch_pred)
    #gradients
    grads =tape.gradient(loss_value, modelh20.trainable_weights)
    #norm of grads
    gradsNorm.append( np.sqrt( sum( [np.sum(np.square(g.numpy())) for g in grads])))
    batch_loss.append(loss_value.numpy())
    optimizer.apply_gradients( zip(grads,modelh20.trainable_weights))
  epoch_loss_value= modelh20.evaluate(X_train,y_train,verbose=0)
  epoch_loss.append(epoch_loss_value)

  val_loss_value = modelh20.evaluate(X_test,y_test,verbose=0)
  val_loss.append(val_loss_value)
  print("epoch_loss: %.4f - val_loss:%.4f" %(float(epoch_loss_value),float(val_loss_value)))

  if val_loss_value < best_val_loss:
    best_val_loss=val_loss_value
    modelh20.save("bestModelh20")

np.array(gradsNorm)

np.array(epoch_loss)

bestModelh20 = tf.keras.models.load_model('BestModel.h20')

modelh20.evaluate(X_test,y_test)

bestModelh20.evaluate(X_test,y_test)

#h* hidden layer activities
bestModelh20.layers[0](X_test).numpy().shape

best_val_loss
#0.5326284170150757

import statistics
from statistics import mean
mean(Monitor.history['val_loss'])
#48.517317899068196

"""**End:h=20**

**Start: Plots h=20**
"""

# INsert Plots here please
fig, ax = plt.subplots(2, figsize= (10,6))
ax[0].scatter(range(0,len(gradsNorm)),gradsNorm)
ax[0].set_ylabel('MSE')
ax[1].scatter(range(0,len(batch_loss)), batch_loss)
plt.xlabel('Epoch')
ax[1].set_ylabel('Norm of Gradient MSE')
plt.savefig('h20_batch_labeled.png')

"""**END: Plots h=20**

**Start h =40**
"""

modelh40 =Sequential()
modelh40.add(Dense(40, activation="relu", input_shape=(16,)))
modelh40.add(Dense(1, activation="relu"))

modelh40.summary()

#weights matrix for model h=40
modelh40.layers[0].get_weights()

modelh40.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())

#For recording training loss over the whole training set at the end of each epoch

class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())

MyMonitor = MyHistory()

# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h40',monitor='val_loss',save_best_only=True)

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)

#Train the model and store validation loss history in Monitor
# Value of epochs should vary according to your own training results.
Monitor = modelh40.fit(X_train,y_train,epochs=8000,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])

#Train loss vs Val_loss for h=40 
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(8000)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.savefig('h40_val_loss.png')

# Average of gm stock
AvgGm=df['Y(T)'].mean()
# Converting MSe too RMSE/Avg(GM) 
train_loss1=list(np.sqrt(train_loss))
tlavggm=[x/AvgGm for x in train_loss1]
val_loss1=list(np.sqrt(val_loss))
vlavggm=[x/AvgGm for x in val_loss1]
train_loss1 = tlavggm[0:50]
val_loss1 = vlavggm[0:50]
train_loss2 = tlavggm[50:1000]
val_loss2 = vlavggm[50:1000]
train_loss3 = tlavggm[1000:4000]
val_loss3 = vlavggm[1000:4000]
train_loss4 = tlavggm[4000:8000]
val_loss4 = vlavggm[4000:8000]
fig, (ax1, ax2,ax3,ax4) = plt.subplots(1, 4,figsize=(20,6))
fig.suptitle('')
x1=range(0,50)
x2 = range(50,1000)
x3= range(1000,4000)
x4 = range(4000,8000)
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)
ax3.plot(x3, train_loss3)
ax3.plot(x3, val_loss3)
ax4.plot(x4, train_loss4)
ax4.plot(x4,val_loss4)
xc=range(8000)

modelh40.evaluate(X_test,y_test)

best_modelh40 =load_model('BestModel.h40')
best_modelh40.evaluate(X_test,y_test)

ypred = best_modelh40.predict(X_test)

x_testgmlist=[]
for i in range(len(y_test)):
  x_testgmlist.append(X_testcopy[i][1])
  
  i=i+1

y_testlist=list(y_test)
realdiff=[]
for i in range(len(y_test)):
  x=y_testlist[i] - x_testgmlist[i]
  realdiff.append(x)
  i=i+1

realdiffbin=[]
for i in range(len(realdiff)):
  if realdiff[i]>0:
    x=1
  else:
    x=0
  realdiffbin.append(x)
  i=i+1
#realdiffbin = binary list for weather the price went up or down on day T+1

y_predlist=ypred.tolist()
preddiff=[]
for i in range(len(y_test)):
  x=list(y_predlist[i] - x_testgmlist[i])
  preddiff.append(x)
  i=i+1

preddiffbin=[]
for i in range(len(preddiff)):
  if preddiff[i][0]>0:
    x=1
  else:
    x=0
  preddiffbin.append(x)
  i=i+1
#preddiffbin dinary list of predictions out model made, 1 means increase, 0 means decrease

## total of the predictions and actual, if total = 0 or 2 prediction was correct, if total equals to 1 it was wrong

totalpredact=[]

for i in range(len(preddiffbin)):
  x=preddiffbin[i]+realdiffbin[i]
  totalpredact.append(x)
  i=i+1
totalpredact.count(1)

#Accuracy for h=40
(len(totalpredact)-totalpredact.count(1))/len(totalpredact)

y_testlist=list(y_test)
realdiff=[]
for i in range(len(y_test)):
  x=y_testlist[i] - x_testgmlist[i]
  realdiff.append(x)
  i=i+1

realdiffbin=[]
for i in range(len(realdiff)):
  if realdiff[i]>0:
    x=1
  else:
    x=0
  realdiffbin.append(x)
  i=i+1
#realdiffbin = binary list for weather the price went up or down on day T+1

y_predlist=ypred.tolist()
preddiff=[]
for i in range(len(y_test)):
  x=list(y_predlist[i] - x_testgmlist[i])
  preddiff.append(x)
  i=i+1

preddiffbin=[]
for i in range(len(preddiff)):
  if preddiff[i][0]>0:
    x=1
  else:
    x=0
  preddiffbin.append(x)
  i=i+1
#preddiffbin dinary list of predictions out model made, 1 means increase, 0 means decrease

## total of the predictions and actual, if total = 0 or 2 prediction was correct, if total equals to 1 it was wrong

totalpredact=[]

for i in range(len(preddiffbin)):
  x=preddiffbin[i]+realdiffbin[i]
  totalpredact.append(x)
  i=i+1
totalpredact.count(1)

#Accuracy for h=2
(len(totalpredact)-totalpredact.count(1))/len(totalpredact)

type(predict)

type(df['GM(T-3)'])

X_test.head()

optimizer=optimizers.Adam(learning_rate=0.01)
loss_fn=losses.MeanSquaredError()

modelh40.compile(optimizer=optimizer,loss=loss_fn)

batch_size=25

train_dataset= tf.data.Dataset.from_tensor_slices((X_train, y_train))

train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
batch_loss=[]
gradsNorm=[]
epoch_loss=[]
val_loss=[]
best_val_loss=modelh40.evaluate(X_test,y_test,verbose=0)
epochs=2000

for epoch in range(epochs):
  print("\nStart of epoch %d" %(epoch,))
  for step, (x_batch_train, y_batch_train) in enumerate (train_dataset):
    with tf.GradientTape() as tape:
      y_batch_pred=modelh40(x_batch_train, training=True)
      loss_value=loss_fn(y_batch_train,y_batch_pred)
    #gradients
    grads =tape.gradient(loss_value, modelh40.trainable_weights)
    #norm of grads
    gradsNorm.append( np.sqrt( sum( [np.sum(np.square(g.numpy())) for g in grads])))
    batch_loss.append(loss_value.numpy())
    optimizer.apply_gradients( zip(grads,modelh40.trainable_weights))
  epoch_loss_value= modelh40.evaluate(X_train,y_train,verbose=0)
  epoch_loss.append(epoch_loss_value)

  val_loss_value = modelh40.evaluate(X_test,y_test,verbose=0)
  val_loss.append(val_loss_value)
  print("epoch_loss: %.4f - val_loss:%.4f" %(float(epoch_loss_value),float(val_loss_value)))

  if val_loss_value < best_val_loss:
    best_val_loss=val_loss_value
    modelh40.save("bestModelh40")

grads

np.array(gradsNorm)

np.array(epoch_loss)

bestModelh40 = tf.keras.models.load_model('BestModel.h40')

modelh40.evaluate(X_test,y_test)

bestModelh40.evaluate(X_test,y_test)
#1.1612101793289185

best_val_loss
#0.5283870697021484

#h* hidden layer activities
#h* hidden layer activities
arr = bestModelh40.layers[0](X).numpy()


h40_avg_activities = arr.mean(axis=0)
print(h40_avg_activities)

len(h40_avg_activities)
val, idx_max = max((val, idx) for (idx, val) in enumerate(h40_avg_activities))
print(idx_max)
val, idx_min = min((val, idx) for (idx, val) in enumerate(h40_avg_activities))
print(idx_min)
a = h40_avg_activities.tolist()
a.sort()
print(a)
plt.plot(range(40),a)
plt.xlabel('Neuron')
plt.ylabel('Activity')
plt.savefig('neuronactivity.png')
#3 most inactive:  0.0, 0.0, 0.0 (has index 0)
#3 most active: 485.7743835449219, 474.0765075683594, 447.3477478027344  (has index 32)

#weights matrix for best model h=40
bestModelh40.layers[0].get_weights()

"""**Start: Weights for h=40 most active neuron**"""

#weights matrix for model h=40
wts=bestModelh40.layers[0].get_weights()
wtstopneo=[]

for i in range(16):
  wtstopneo.append(wts[0][i][32])
wtstopneo

"""**End:Weights for h=40 most active neuron**

**Start: Weights for h=40 most inactive neuron**
"""

#weights matrix for model h=40
wts=bestModelh40.layers[0].get_weights()
wtstopneo=[]

for i in range(16):
  wtstopneo.append(wts[0][i][0])
wtstopneo

"""**END: Weights for h=40 most inactive neuron**



"""

print(max(h55_avg_activities))

min(Monitor.history['val_loss'])
#1.2567142248153687

"""**End: h=40**

**Start: Plots for h=40**
"""

# Insert Plots here 
# INsert Plots here please
fig, ax = plt.subplots(2, figsize= (10,6))
ax[0].scatter(range(0,len(gradsNorm)),gradsNorm)
ax[0].set_ylabel('MSE')
ax[1].scatter(range(0,len(batch_loss)), batch_loss)
plt.xlabel('Epoch')
ax[1].set_ylabel('Norm of Gradient MSE')
plt.savefig('h40_batch_labeled.png')



"""**End: Plots for h=40**

**Start: h=55**
"""

modelh55 =Sequential()
modelh55.add(Dense(55, activation="relu", input_shape=(16,)))
modelh55.add(Dense(1, activation="relu"))

modelh55.summary()

#weights matrix for model h=55
modelh55.layers[0].get_weights()

modelh55.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())

#For recording training loss over the whole training set at the end of each epoch

class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())

MyMonitor = MyHistory()

# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h55',monitor='val_loss',save_best_only=True)

#Train the model and store validation loss history in Monitor
# Value of epochs should vary according to your own training results.
Monitor = modelh55.fit(X_train,y_train,epochs=8000,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])



#Train loss vs Val_loss for h=2
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(8000)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.savefig('h55_val_loss.png')

# Average of gm stock
AvgGm=df['Y(T)'].mean()
# Converting MSe too RMSE/Avg(GM) 
train_loss1=list(np.sqrt(train_loss))
tlavggm=[x/AvgGm for x in train_loss1]
val_loss1=list(np.sqrt(val_loss))
vlavggm=[x/AvgGm for x in val_loss1]
train_loss1 = tlavggm[0:50]
val_loss1 = vlavggm[0:50]
train_loss2 = tlavggm[50:1000]
val_loss2 = vlavggm[50:1000]
train_loss3 = tlavggm[1000:4000]
val_loss3 = vlavggm[1000:4000]
train_loss4 = tlavggm[4000:8000]
val_loss4 = vlavggm[4000:8000]
fig, (ax1, ax2,ax3,ax4) = plt.subplots(1, 4,figsize=(20,6))
fig.suptitle('')
x1=range(0,50)
x2 = range(50,1000)
x3= range(1000,4000)
x4 = range(4000,8000)
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)
ax3.plot(x3, train_loss3)
ax3.plot(x3, val_loss3)
ax4.plot(x4, train_loss4)
ax4.plot(x4,val_loss4)
xc=range(8000)

modelh55.evaluate(X_test,y_test)

best_modelh55 =load_model('BestModel.h55')
best_modelh55.evaluate(X_test,y_test)

X_test[1]

#Predictions on x test
ypred=best_modelh55.predict(X_test)

x_testgmlist=[]
for i in range(len(y_test)):
  x_testgmlist.append(X_testcopy[i][1])
  
  i=i+1



y_testlist=list(y_test)
realdiff=[]
for i in range(len(y_test)):
  x=y_testlist[i] - x_testgmlist[i]
  realdiff.append(x)
  i=i+1


realdiffbin=[]
for i in range(len(realdiff)):
  if realdiff[i]>0:
    x=1
  else:
    x=0
  realdiffbin.append(x)
  i=i+1
#realdiffbin = binary list for weather the price went up or down on day T+1

y_predlist=ypred.tolist()
preddiff=[]
for i in range(len(y_test)):
  x=list(y_predlist[i] - x_testgmlist[i])
  preddiff.append(x)
  i=i+1

preddiffbin=[]
for i in range(len(preddiff)):
  if preddiff[i][0]>0:
    x=1
  else:
    x=0
  preddiffbin.append(x)
  i=i+1
#preddiffbin dinary list of predictions out model made, 1 means increase, 0 means decrease

## total of the predictions and actual, if total = 0 or 2 prediction was correct, if total equals to 1 it was wrong

totalpredact=[]

for i in range(len(preddiffbin)):
  x=preddiffbin[i]+realdiffbin[i]
  totalpredact.append(x)
  i=i+1
totalpredact.count(1)

#Accuracy for h=55
(len(totalpredact)-totalpredact.count(1))/len(totalpredact)

optimizer=optimizers.Adam(learning_rate=0.01)
loss_fn=losses.MeanSquaredError()

modelh55.compile(optimizer=optimizer,loss=loss_fn)

batch_size=25

train_dataset= tf.data.Dataset.from_tensor_slices((X_train, y_train))

train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
batch_loss=[]
gradsNorm=[]
epoch_loss=[]
val_loss=[]
best_val_loss=modelh55.evaluate(X_test,y_test,verbose=0)
epochs=150

best_val_loss
#0.8882755637168884



for epoch in range(epochs):
  print("\nStart of epoch %d" %(epoch,))
  for step, (x_batch_train, y_batch_train) in enumerate (train_dataset):
    with tf.GradientTape() as tape:
      y_batch_pred=modelh55(x_batch_train, training=True)
      loss_value=loss_fn(y_batch_train,y_batch_pred)
    #gradients
    grads =tape.gradient(loss_value, modelh55.trainable_weights)
    #norm of grads
    gradsNorm.append( np.sqrt( sum( [np.sum(np.square(g.numpy())) for g in grads])))
    batch_loss.append(loss_value.numpy())
    optimizer.apply_gradients( zip(grads,modelh55.trainable_weights))
  epoch_loss_value= modelh55.evaluate(X_train,y_train,verbose=0)
  epoch_loss.append(epoch_loss_value)

  val_loss_value = modelh55.evaluate(X_test,y_test,verbose=0)
  val_loss.append(val_loss_value)
  print("epoch_loss: %.4f - val_loss:%.4f" %(float(epoch_loss_value),float(val_loss_value)))

  if val_loss_value < best_val_loss:
    best_val_loss=val_loss_value
    modelh55.save("bestModelh55")

grads

np.array(gradsNorm)

np.array(epoch_loss)

bestModelh55 = tf.keras.models.load_model('bestModelh55')

modelh55.evaluate(X_test,y_test)

bestModelh55.evaluate(X_test,y_test)



import statistics
from statistics import mean
a = Monitor.history['val_loss']
min(a)

"""**Start: Plots for h=55**"""

# Insert Plots here

"""**End: Plots for h=55**"""

plt.plot(MyMonitor.real_loss,label="train")
plt.plot(Monitor.history['val_loss'],label="test")
plt.legend()
fig, ax = plt.subplots(2, figsize= (10,6))
ax[0].scatter(range(0,len(gradsNorm)),gradsNorm)
ax[0].set_ylabel('MSE')
ax[1].scatter(range(0,len(batch_loss)), batch_loss)
plt.xlabel('Epoch')
ax[1].set_ylabel('Norm of Gradient MSE')
plt.savefig('h55_batch_labeled.png')

"""**Additional Models to try to fund a better model**"""

modelh =Sequential()
modelh.add(Dense(40, activation="relu", input_shape=(16,)))
modelh.add(Dense(20,activation="relu"))
modelh.add(Dense(1, activation="relu"))

modelh.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())
class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())
 
MyMonitor = MyHistory()
 
# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h',monitor='val_loss',save_best_only=True)
Monitor = modelh.fit(X_train,y_train,epochs=150,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])
best_modelh =load_model('BestModel.h')
best_modelh.evaluate(X_test,y_test)

modelh =Sequential()
modelh.add(Dense(40, activation="relu", input_shape=(16,)))
modelh.add(Dense(20,activation="relu"))
modelh.add(Dense(10,activation="relu"))
modelh.add(Dense(1, activation="relu"))

modelh.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())
class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())
 
MyMonitor = MyHistory()
 
# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h',monitor='val_loss',save_best_only=True)
Monitor = modelh.fit(X_train,y_train,epochs=150,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])
best_modelh =load_model('BestModel.h')
best_modelh.evaluate(X_test,y_test)

modelh =Sequential()
modelh.add(Dense(40, activation="relu", input_shape=(16,)))
modelh.add(Dense(20,activation="relu"))
modelh.add(Dense(1, activation="relu"))

modelh.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())
class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())
 
MyMonitor = MyHistory()
 
# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h',monitor='val_loss',save_best_only=True)
Monitor = modelh.fit(X_train,y_train,epochs=100,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])
best_modelh =load_model('BestModel.h')
best_modelh.evaluate(X_test,y_test)

modelh =Sequential()
modelh.add(Dense(40, activation="relu", input_shape=(16,)))
modelh.add(Dense(20,activation="relu"))
modelh.add(Dense(1, activation="relu"))

modelh.compile(optimizer=optimizers.RMSprop(), loss=losses.MeanSquaredError())
class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())
 
MyMonitor = MyHistory()
 
# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h',monitor='val_loss',save_best_only=True)
Monitor = modelh.fit(X_train,y_train,epochs=150,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])
best_modelh =load_model('BestModel.h')
best_modelh.evaluate(X_test,y_test)

modelh =Sequential()
modelh.add(Dense(40, activation="relu", input_shape=(16,)))
modelh.add(Dense(20,activation="relu"))
modelh.add(Dense(1, activation="relu"))

modelh.compile(optimizer=optimizers.SGD(), loss=losses.MeanSquaredError())
class MyHistory(callbacks.Callback):
  def on_train_begin(self, logs=None):
    self.real_loss=[]
  def on_epoch_end(self, epoch, logs=None):
    Ypred=self.model.predict(X_train).flatten()
    loss_value = self.model.loss(y_train,Ypred)
    self.real_loss.append(loss_value.numpy())
 
MyMonitor = MyHistory()
 
# for saving the best model during the whole training process
checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h',monitor='val_loss',save_best_only=True)
Monitor = modelh.fit(X_train,y_train,epochs=300,batch_size=25,validation_data=(X_test, y_test),callbacks = [checkpointer, MyMonitor])
best_modelh =load_model('BestModel.h')
best_modelh.evaluate(X_test,y_test)

